---
title: "Movie Lens Project - Iakovos Tselentis"
author: "Iakovos Tselentis"
date: "10/17/2020"
output:
  pdf_document: 
    latex_engine: xelatex
    
fig_width: 6 
fig_height: 4 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(cache = TRUE)
Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS=TRUE)

```

# Introduction

## Summary

This project is part of the Capstone course for HarvardX's *Professional Certificate in Data Science* program. The goal is to create a basic movie recommendation system that predicts how many stars a user will give to a specific movie using skills that were taught in the previous 8 courses of the program. The accuracy of the algorithm will be evaluated using the *Root-Mean-Square-Error (RMSE)*, which is defined as follows:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

We will now also define the function above in R using the code below:
```{r RMSE_function1, echo = TRUE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

In the following sections we are going to:

- Explore our dataset

- Split it into the edx(Training) and validation(Test) set
    
- Use the edx set to create and test our models
  
- Implement the best performing model in the validation set

- Summarize the report, draw conclusions and outline its limitations


Before all that we make sure that we have all the packages needed to complete the project.

```{r Package installation, echo = TRUE, message =FALSE, warning=FALSE}

if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(ggthemes)
library(stringr)
memory.limit(60000)
```

# Dataset Exploration

## Download

The dataset for this project is called Movie Lens and is provided by *GroupLens Reasearch Lab*.The dataset is free, and accessible via the following [*link*](https://grouplens.org/datasets/movielens/10m/). In the next code chunks we are going to load the Movie Lens dataset. Then we partition it in two sets, the edx and the final hold-out set. The edx set is going to be used in the next sections to train and test our model. Once we decide on the optimal model we are going to implement it in the validation set(final hold-out set) and compare it with the actual ratings to calculate the final RMSE.


```{r Download, echo=TRUE, message=FALSE, warning=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                          genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Exploration


Let's first take a look at the top 6 rows of the dataset:
```{r  exploration1, warning=FALSE, echo=FALSE}
head(edx)
```

The edx dataset has `r nrow(edx)` rows and `r ncol (edx)` columns.Each row represents a rating given by one user to one movie. But did every user rate every movie?
Taking a random sample of 100 users and 100 movies we see that our dataset is quite sparse.

```{r exploration2, warning=FALSE, echo=FALSE}

users100 <- sample(unique(edx$userId),100)

edx%>%filter(userId %in% users100) %>%
  select(userId,movieId,rating) %>%
  mutate(rating=1) %>%
  spread(movieId,rating) %>% select(sample(ncol(.),100)) %>%
  as.matrix() %>% t(.) %>%
  image(1:100,1:100,.,xlab="Movies",ylab="Users") %>%
  abline (h=0:100+0.5,v=0:100+0.5,col="grey")
```

Also, we can notice that our dataset needs to be slightly manipultated in order to contain useful information for our analysis. First we are going to extract the year from the title using regex expressions, convert the timestamp to a date and extract the year of the rating, as well as create a new column with the movie age on rating.


``` {r Data Manipulation, warning= FALSE, echo = FALSE}
#Converting the timestamp to year and Extracting the Realease Year from the Title

edx_A<- edx%>% mutate(year_released=(str_extract(title,"\\s\\(\\d{4}\\)")))%>%
  mutate(year_released= str_replace(year_released,"\\s\\((\\d{4})\\)","\\1")) %>%
  mutate(year_rated=(year(as_datetime(timestamp))))

head(edx_A)

#Doing the same for the Validation set
validation_A<- validation%>% mutate(year_released=(str_extract(title,"\\s\\(\\d{4}\\)")))%>%
  mutate(year_released= str_replace(year_released,"\\s\\((\\d{4})\\)","\\1"))%>%
  mutate(year_rated=(year(as_datetime(timestamp))))
```

## Graphs


``` {r graphs1, warning = FALSE, echo = FALSE}

#Distribution of Movie Ratings

edx_A%>%count(title)%>%ggplot(aes(n))+  geom_histogram(bins=30,color="black") +geom_vline(xintercept = 843,col='red',linetype="dashed")+  scale_x_log10()+ ggtitle("Number of Movie Ratings")+theme_economist_white()

```

``` {r graphs2, warning = FALSE, echo = FALSE}
#Distribution of User Ratings
edx_A%>% count(userId)%>% ggplot(aes(n))+
  geom_histogram(bins=30,color="black") + geom_vline(xintercept = 129, col = "red", linetype = "dashed")+
  scale_x_log10()+ ggtitle("Number of User Ratings")+theme_economist_white()
```

``` {r graphs3, warning = FALSE, echo = FALSE}

#Ratings Distribution
ggplot(edx_A, aes(rating)) + geom_histogram(color = "black",binwidth = 0.2) + scale_x_continuous(breaks = seq(0.5, 5, 0.5)) + geom_vline(xintercept = mean(edx$rating), col = "red", linetype = "dashed") + labs(title = "Distribution of ratings",  x = "Ratings Scale", y = "Sum Of Rating")+ theme_economist_white()

```


# Analysis

In this section we are going to start building our models that predict movie ratings, using the *edx* set. To predict the ratings we are gradually going to introduce new factors in each model. By adding new factors we try to reduce the ambiguity caused by the residuals of the previous model and hopefully increase the accuracy of our prediction.

Before that however, we need to partition our edx set into a train and a test set. We are going to train each model on the train set and implement the prediction on the test set to reduce the probability of overfitting our model.

``` {r analysis1, warning = FALSE, echo = TRUE}

#Data partition
set.seed(1,sample.kind="Rounding")
partindex<- createDataPartition(edx_A$rating,p=0.1,list=FALSE)
train_set<-edx_A[-partindex,]
test_set<-edx_A[partindex,]
#we also remove all the movies from the test set that do not appear in the train set
test_set<- test_set %>%  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

```

## Model 1: Just the Average

Our first model predicts that each user will give each movie the average movie rating. 
This rating according to our edx train set is at 3.5 stars.

$$ Y_{u,i} = \mu + \varepsilon_{u,i} $$

``` {r model1, warning = FALSE, echo = TRUE}

mu <- mean(train_set$rating)
naive_rmse<-RMSE(test_set$rating,mu)
RMSE_by_method <- data_frame(method = "Just the average", RMSE = round(naive_rmse,digits=3))
RMSE_by_method%>%knitr::kable()

```


## Model 2: Movie Effects
Our Previous model's predictions deviate from the actual ratings by more than one star on average. For this reason we are going to introduce one more factor that will hopefully explain the residuals of the 'Just the Average' Model. This factor is going to explain the effect of each movie. 

$$ Y_{u,i} = \mu + b_{i} + \varepsilon_{u,i} $$


```{r model2, warning = FALSE, echo = TRUE}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))


qplot(b_i, data = movie_avgs, bins = 10, color = I("black"))+
  theme_economist_white()+
  ggtitle("Movie Effect Distribution")

test_set<- test_set %>% 
  left_join(movie_avgs, by='movieId')


predicted_ratings <- mu + test_set %>%
  pull(b_i)



model_2_rmse <- RMSE(test_set$rating,predicted_ratings)
RMSE_by_method<-bind_rows(RMSE_by_method,data_frame(method="Movie Effect Model",RMSE=model_2_rmse))
RMSE_by_method%>%knitr::kable()
```

From the graph above we see that most of the movies, as expected revolve around 0 deviation from the mean (3.5 stars), but there are certain movies that have on average much better or much worse rating. The movie effects (b_i) factor helps us reduce the RMSE to 0.94

## Model 3: Movie Effects + User Effects
Let's see if we can further decrease RMSE by adding the user effects. This factor will show the tendency that users have to rate movies higher or lower than the average rating.

$$ Y_{u,i} = \mu + b_{i} +  b_{u} + \varepsilon_{u,i} $$

```{r model3, warning = FALSE, echo = TRUE}
train_set<- train_set%>% mutate(mu= mu) %>% left_join(movie_avgs,by='movieId') 
user_avgs <-train_set %>% group_by(userId)%>%
  summarise(b_u= mean(rating-mu-b_i))

test_set<- test_set %>% 
  left_join(user_avgs, by='userId')


predicted_ratings <- mu + test_set %>%
  pull(b_i) + test_set %>%
  pull(b_u)


model_3_rmse <- RMSE(test_set$rating,predicted_ratings)
RMSE_by_method<-bind_rows(RMSE_by_method,data_frame(method="Movie Effect & User Effect Model",RMSE=model_3_rmse))
RMSE_by_method%>%knitr::kable()
```

Our prediction has significantly improved again, with the RMSE reduced to 0.865

## Model 4: Movie Effects + User Effects + Release Year Effect

In the next model we are adding the release year effect. By adding a new factor in our model, we will explore if movies released in a certain year had a better average rating, capturing the 'golden eras' of cinema in our model. First, let's graph the average ratings for each year.

```{r model4a, warning = FALSE, echo = FALSE}

#Exploring the effect of premier date in rating
edx_A%>% group_by(year_released) %>%  summarise(rating=mean(rating))%>% 
  ggplot(aes(year_released,rating)) + geom_point() +theme_economist_white()+theme(axis.text.x=element_text(angle=90,hjust =1))
```
It seems that the data follow a pattern, movies after the 80s have received a lower average rating. While, movies released in the 30s and 70s have a better average rating. So we are now going to implement the new factor in our model as described in the equation below:

$$ Y_{u,i} = \mu + b_{i} +  b_{u} + bry_{i} + \varepsilon_{u,i} $$


```{r model4b, warning = FALSE, echo = TRUE}


train_set<- train_set%>%  left_join(user_avgs,by='userId') 

release_year_avgs <-train_set %>% group_by(year_released)%>%
  summarise(b_ry= mean(rating-mu-b_i-b_u))


#implementing year averages in the test set

test_set<- test_set%>% 
  left_join(release_year_avgs, by='year_released')

predicted_ratings <- mu + test_set %>%
  pull(b_i) + test_set %>%
  pull(b_u) + test_set %>%pull(b_ry)

model_4_rmse <- RMSE(test_set$rating,predicted_ratings)
RMSE_by_method<-bind_rows(RMSE_by_method,data_frame(method="Movie Effect & User Effect & Release Year Effect",RMSE=model_4_rmse))
train_set<- train_set%>% mutate(mu= mu) %>% left_join(release_year_avgs,by='year_released') 

RMSE_by_method%>%knitr::kable()

```

The new model has marginally improved our prediction, reducing the RMSE to 0.864

## Regularization


Our previous models have yielded good results, however we can still improve the predictions. 
Let's see where are the biggest errors. 
```{r reg1, warning = FALSE, echo = FALSE}
test_set%>%
  mutate(residual= rating-(mu+b_i+b_u+b_ry),prediction=mu+b_i+b_u+b_ry) %>%
  arrange(desc(residual)) %>%
  select(title,residual,prediction,rating) %>% slice(1:10) %>% knitr::kable()
```

Our biggest errors are for movies that are largely unpopular. 

Let's see the top 10 worst and best movies using the movie according to our movie bias factor and their respective ratings
```{r reg2, warning = FALSE, echo = FALSE}
#First make a table with the distinct movies and their respective bi from movie avgs
distinct_movies  <- edx %>% 
  select(movieId, title) %>%
  distinct()

meanNratings<-  train_set %>% dplyr::count(movieId)%>% summarise(mean=mean(n))

##Worse Ratings
train_set %>% dplyr::count(movieId)%>% left_join(movie_avgs)%>%left_join(distinct_movies,by='movieId')%>%arrange(desc(b_i)) %>% select(title,b_i,n) %>% slice(1:10)%>%knitr::kable()

##Best Ratings
train_set %>% dplyr::count(movieId)%>% left_join(movie_avgs)%>%left_join(distinct_movies,by='movieId')%>%arrange(b_i) %>% select(title,b_i,n) %>% slice(1:10)%>%knitr::kable()
```

We see that the number of ratings for the top 10 and worst 10 movies is quite lower than the average number of ratings in our sample which is `r meanNratings`.


Following the same process for our other user effect we see that the outliers are users that have given less movie ratings that the average which is `r train_set %>% dplyr::count(userId)%>% summarise(mean=mean(n))`.
```{r  reg3, warning = FALSE, echo = FALSE}

#Second make a table with the distinct users and their respective bu from user avgs
distinct_users  <- edx %>% 
  select(userId) %>%
  distinct()


train_set %>% dplyr::count(userId)%>% left_join(user_avgs)%>%left_join(distinct_users,by='userId')%>%arrange(b_u) %>% select(userId,b_u,n) %>% slice(1:10)%>%knitr::kable()

train_set %>% dplyr::count(userId)%>% left_join(user_avgs)%>%left_join(distinct_users,by='userId')%>%arrange(desc(b_u)) %>% select(userId,b_u,n) %>% slice(1:10)%>%knitr::kable()

```

Similarly, the Release Years that have the best average rating according to our model are years with very few observations.
``` {r reg4, warning = FALSE, echo = FALSE}
distinct_years  <- edx_A %>% 
  select(year_released) %>%distinct()

train_set %>% dplyr::count(year_released)%>% left_join(release_year_avgs)%>%left_join(distinct_years,by='year_released')%>%arrange(b_ry) %>% select(year_released,b_ry,n) %>% slice(1:10)%>%knitr::kable()

train_set %>% dplyr::count(year_released)%>% left_join(release_year_avgs)%>%left_join(distinct_years,by='year_released')%>%arrange(desc(b_ry)) %>% select(year_released,b_ry,n) %>% slice(1:10)%>%knitr::kable()
```

### Penalty term

We see that the biggest errors are generate by factors with too few observations. For this reason we are going to introduce a term $\lambda$ that will penalize large estimates from small samples. All factors(biases) will be re-estimated. This time instead of using the simple mean for each factor we are going to divide the sum of each group of observations with the sample size plus the $\lambda$ term. This way, groups with large sample size are not going to be affected, but groups with small sample sizes are going to be pulled towards 0 deviation.


``` {r model5, warning = FALSE, echo = FALSE}
#####Regularization for movie, user and release year effects. 
##Cross Validation.

lambdas<- seq (0,10,0.5)
mu <- mean(train_set$rating)

rmses <- sapply(lambdas, function(l){
  b_i_r <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_r = sum(rating - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_r, by='movieId') %>% 
    mutate(pred = mu + b_i_r) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})

plot(lambdas,rmses)
data_frame(lambdas=lambdas, rmses=rmses)[which.min(rmses),]
min(rmses)
model_1_rmse

lambdas<- seq (0,10,0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  
  b_i_r <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_r = sum(rating - mu)/(n()+l))
  
  b_u_r <- train_set %>% left_join(b_i_r,by="movieId")%>%
    group_by(userId) %>% 
    summarize(b_u_r = sum(rating - mu-b_i_r)/(n()+l))
  
  b_ry_r <- train_set %>% left_join(b_u_r,by="userId")%>%
    left_join(b_i_r,by="movieId")%>%
    group_by(year_released) %>% 
    summarize(b_ry_r = sum(rating - mu-b_i_r-b_u_r)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i_r,by="movieId") %>%
    left_join(b_u_r,by="userId") %>% 
    left_join(b_ry_r,by="year_released")%>%  
    #left_join(b_ma_r,by='movie_age_on_rating')%>%
    mutate(pred = mu + b_i_r+b_u_r+b_ry_r)%>% .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
plot(lambdas,rmses)
```
It  seems that the best RMSE is return when the penatly term lambda is set to 4.75.


Now, let's compare all models.
``` {r comparison, warning = FALSE, echo = FALSE}
model_5_rmse <- data.frame(rmses)[which.min(rmses),]
RMSE_by_method<-bind_rows(RMSE_by_method,data_frame(method="Regularized Movie Effect & User Effect & Release Year Effect",RMSE=model_5_rmse))
RMSE_by_method%>%knitr::kable()
```


# Results

After comparing all models we reach to the conclusion that the 'Regularized Movie Effect & User Effect & Release Year Effect' model with the penalty error set at 4.75 yields the best RMSE from all previous models. Now we can implement it in our validation set and calculate the final RMSE. 
``` {r results, warning = FALSE, echo = TRUE}

l<- 4.75


mu <- mean(edx_A$rating)

b_i_r <- edx_A %>% 
  group_by(movieId) %>% 
  summarize(b_i_r = sum(rating - mu)/(n()+l))

b_u_r <- edx_A %>% left_join(b_i_r,by="movieId")%>%
  group_by(userId) %>% 
  summarize(b_u_r = sum(rating - mu-b_i_r)/(n()+l))

b_ry_r <- edx_A %>% left_join(b_u_r,by="userId")%>%
  left_join(b_i_r,by="movieId")%>%
  group_by(year_released) %>% 
  summarize(b_ry_r = sum(rating - mu-b_i_r-b_u_r)/(n()+l))


predicted_ratings <- validation_A %>% 
  left_join(b_i_r,by="movieId") %>%
  left_join(b_u_r,by="userId") %>% 
  left_join(b_ry_r,by="year_released")%>%  
  mutate(pred = mu + b_i_r+b_u_r+b_ry_r)%>% .$pred



FinalRMSE <- RMSE(validation$rating,predicted_ratings)
```

Our final RMSE calculation is `r FinalRMSE`.  



# Conclusion

In this short report we have demonstrated how using big data and simple models can help us create strong algorithms for predicting results and reccommending content. We have touched upon the basic principles of data science. We loaded a dataset from an online source, we cleaned it and used basic text mining functions to extract important information, we graphed our data and introduced gradually 3 factors and a penalty error that manage to predict the ratings of users for movies to a satisfactory level. 

The analysis and the prediction could be significantly more accurate if we had used machine learning techniques to factorize the matrix, such as singular value decomposition or principal component analysis. These techniques are also what used by the famous reccommendation algorithms of companies such as Netflix, Amazon, Google and Facebook.However, this would require significantly more time and computing power.
